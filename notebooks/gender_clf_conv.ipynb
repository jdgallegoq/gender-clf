{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MLP with image augmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ol>1. Loading the dataset</ol>\n",
    "<ol>2. Pre-processing the data</ol>\n",
    "<ol>3. Creating training and validation set</ol>\n",
    "<ol>4. Defining the model architecture</ol>\n",
    "<ol>5. Compiling the model</ol>\n",
    "<ol>6. Training the model</ol>\n",
    "<ol>7. Evaluating model performance</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version:  2.9.2\n",
      "keras version:  2.9.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import keras\n",
    "print('tensorflow version: ', tf.__version__)\n",
    "print('keras version: ', keras.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device mapping:\n",
      "/job:localhost/replica:0/task:0/device:GPU:0 -> device: 0, name: Quadro P4000, pci bus id: 0000:00:05.0, compute capability: 6.1\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# define sess to use gpu\n",
    "sess = tf.compat.v1.Session(config=tf.compat.v1.ConfigProto(log_device_placement=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary libraries and functions\n",
    "import os\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from keras.layers import Dense, InputLayer\n",
    "from keras.models import Sequential\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "sys.path.append('..')\n",
    "from utils.s3_class import S3Functions\n",
    "s3_funcs = S3Functions(bucket_name='jdgallegoq-pinacle')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create random number generator\n",
    "seed = 42\n",
    "rng = np.random.RandomState(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 12196 entries, 0 to 12195\n",
      "Data columns (total 2 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   image_names  12196 non-null  object\n",
      " 1   class        12196 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 190.7+ KB\n",
      "None\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5227 entries, 0 to 5226\n",
      "Data columns (total 1 columns):\n",
      " #   Column       Non-Null Count  Dtype \n",
      "---  ------       --------------  ----- \n",
      " 0   image_names  5227 non-null   object\n",
      "dtypes: object(1)\n",
      "memory usage: 41.0+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# train data\n",
    "train_data = pd.read_csv(s3_funcs.read_object(key='gender_clf/train/train.csv'))\n",
    "print(train_data.info())\n",
    "\n",
    "# test data\n",
    "test_data = pd.read_csv(s3_funcs.read_object(key='gender_clf/test.csv'))\n",
    "print(test_data.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    6174\n",
       "0    6022\n",
       "Name: class, dtype: int64"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_data['class'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6602aadd2c774071b8dcbabc7cc466cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load images\n",
    "IMAGES_PATH = 'gender_clf/train/images/'\n",
    "\n",
    "X = []\n",
    "not_found = []\n",
    "num_images_read = 5000\n",
    "#for img_name in tqdm(train_data.image_names):\n",
    "for img_name in tqdm(train_data.image_names[:num_images_read]):\n",
    "  img = s3_funcs.read_image(key=IMAGES_PATH+img_name)\n",
    "  X.append(img)\n",
    "\n",
    "X = np.array(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess scaing pixels\n",
    "X = X / X.max()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split dataset\n",
    "y = train_data['class'].values[:num_images_read]\n",
    "X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "    X,\n",
    "    y,\n",
    "    test_size=0.2,\n",
    "    random_state=seed\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model architecture\n",
    "#from tensorflow.keras.preprocessing.image import ImageGenerator\n",
    "from keras.layers import (\n",
    "    InputLayer,\n",
    "    Dense,\n",
    "    Dropout,\n",
    "    BatchNormalization,\n",
    "    Flatten,\n",
    "    Conv2D,\n",
    "    MaxPool2D\n",
    "    )\n",
    "from keras.initializers import glorot_normal\n",
    "from keras.callbacks import (\n",
    "    EarlyStopping,\n",
    "    ModelCheckpoint\n",
    "    )\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "# X train has 3 channels so features are pixels per channel\n",
    "model.add(InputLayer(input_shape=X_train.shape[1:]))\n",
    "model.add(Conv2D(25, (5, 5), activation='relu', strides=(1, 1), padding='valid'))\n",
    "# add maxpools after conv layers\n",
    "model.add(MaxPool2D(pool_size=(4, 4), padding='valid'))\n",
    "model.add(Conv2D(50, (5, 5), activation='relu', strides=(1, 1), padding='valid'))\n",
    "model.add(MaxPool2D(pool_size=(4, 4), padding='valid'))\n",
    "model.add(Conv2D(50, (5, 5), activation='relu', strides=(1, 1), padding='valid'))\n",
    "model.add(MaxPool2D(pool_size=(4, 4), padding='valid'))\n",
    "# flatten so it can be passed to a dense layer\n",
    "model.add(Flatten())\n",
    "# add dense layers\n",
    "model.add(Dense(units=100, activation='sigmoid'))\n",
    "# dropout to avoid overfitting\n",
    "model.add(Dropout(rate=0.6))\n",
    "# add batch normalization to maintain vector values normalized\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dense(units=100, activation='sigmoid'))\n",
    "model.add(Dropout(rate=0.6))\n",
    "model.add(BatchNormalization())\n",
    "# final layer\n",
    "model.add(Dense(units=1), activation='sigmoid')\n",
    "\n",
    "# print summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compile\n",
    "adam = Adam(learning_rate=1e-5, clipvalue=1)\n",
    "model.compile(\n",
    "    loss='binary_crossentropy',\n",
    "    optimizer=adam,\n",
    "    metrics=['accuracy']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "epochs = 10\n",
    "batch_size = 128\n",
    "model_history = model.fit(\n",
    "    x=X_train,\n",
    "    y=y_train,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data = (X_valid, y_valid)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate\n",
    "train_classes = model.predict_classes(X_train)\n",
    "print('Acc on train: ', accuracy_score(y_train, train_classes))\n",
    "\n",
    "val_classes = model.predict_classes(X_valid)\n",
    "print('Acc on val: ', accuracy_score(y_valid, val_classes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# summarize history for loss\n",
    "plt.plot(model_history.history['loss'])\n",
    "plt.plot(model_history.history['val_loss'])\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'validation'],loc='upper left')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
